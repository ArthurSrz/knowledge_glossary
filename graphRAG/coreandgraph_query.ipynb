{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614fc4e2-0e04-49b0-bf7c-05efee48933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.obsidian import ObsidianReader\n",
    "from llama_index.core.memory.chat_memory_buffer import MessageRole\n",
    "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex, VectorStoreIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import Document, PropertyGraphIndex\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import logging\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.callbacks import LlamaDebugHandler\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "from llama_index.core.indices.property_graph import (\n",
    "    SimpleLLMPathExtractor,\n",
    "    SchemaLLMPathExtractor,\n",
    "    DynamicLLMPathExtractor,\n",
    ")\n",
    "import yaml\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    load_indices_from_storage,\n",
    "    load_graph_from_storage,\n",
    ")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9954a1",
   "metadata": {},
   "source": [
    "# Set LLM (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a66b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dotenv()  # Charge les variables depuis le fichier .env\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Modifier ou ajouter une variable d'environnement\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9e96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-4o\", max_tokens=3000)\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7bc2",
   "metadata": {},
   "source": [
    "# Set local LLM for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4b82",
   "metadata": {},
   "source": [
    "# Set LLM for chat  (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d46a1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Ollama(model=\"tinyllama\", request_timeout=120.0)\n",
    "#Settings.llm = llm\n",
    "#Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc9d70",
   "metadata": {},
   "source": [
    "# Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a data governance consultant\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What's your favorite data tool ?\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47252b4e-eb58-453c-9e7d-bab5286ee296",
   "metadata": {},
   "source": [
    "# Load storage contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e8019-e150-4a60-b59d-cac7a4aebcba",
   "metadata": {},
   "source": [
    "## Load vector storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c55a04-2b21-4c05-96fb-f0f9ce26bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"vector\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(\n",
    "        persist_dir=\"vector\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"vector\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b951a-9776-46da-9309-142acfc3f1dd",
   "metadata": {},
   "source": [
    "## Load knowledge graph storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a086ef48-3be3-4a12-b158-4ed50aa2b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"knowledge_graph\"),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(\n",
    "        persist_dir=\"knowledge_graph\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"knowledge_graph\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28317dfb",
   "metadata": {},
   "source": [
    "## Load onto graph storage context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"onto_graph\"),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(\n",
    "        persist_dir=\"onto_graph\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"onto_graph\"),\n",
    ")\n",
    "print(onto_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c3f5b-f0fa-4e10-85fd-9ff7642de428",
   "metadata": {},
   "source": [
    "# Load index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3e85a-8fca-4138-9ff9-fca4637a47e2",
   "metadata": {},
   "source": [
    "## Load vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274006-61a7-42d7-9556-6ddae87d2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_index = load_index_from_storage(vector_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdab09-2c63-4e23-9ca3-79102db74cf0",
   "metadata": {},
   "source": [
    "### Set retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6c3e925-229f-4636-a7b7-73687a3aa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_query_engine = simple_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b32a3",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f57e4-8bc9-440b-a84c-0080cbd412e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_retriever = simple_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = simple_query_engine.query(\n",
    "    \"Quelle m√©thode utiliser pour pr√©dire si un client va faire d√©faut sur son pr√™t bancaire. Fais moi un plan.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014fd60-81a4-4b7f-aef9-f545402e0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242ad8d-20fb-4126-9380-4d51c7e3fa32",
   "metadata": {},
   "source": [
    "## Load graph index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3c260-c349-43f0-b54c-6dac7a12586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_index = load_index_from_storage(graph_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81c1e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = graph_index.get_networkx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nodes\n",
    "num_nodes = len(nx_graph.edges())\n",
    "\n",
    "print(f\"Number of nodes in the knowledge graph: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graph_index.get_networkx_graph()\n",
    "print(g)\n",
    "print(\"Nodes:\", g.nodes())\n",
    "print(\"Edges:\", g.edges())\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "\n",
    "with open(\"knowledge_graph.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(net.generate_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d6f30",
   "metadata": {},
   "source": [
    "### Set retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "330e38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_query_engine = graph_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e323b",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever = graph_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = graph_query_engine.query(\n",
    "    \"Quelle m√©thode utiliser pour pr√©dire si un client va faire d√©faut sur son pr√™t bancaire. Fais moi un plan.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1031a44-f06e-400f-beaa-2065beeee9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dbdaf",
   "metadata": {},
   "source": [
    "## Load onto graph index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb203e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_storage_context = StorageContext.from_defaults(persist_dir=\"onto_graph\")\n",
    "# Load the PropertyGraphIndex from the storage context\n",
    "\n",
    "onto_index = load_index_from_storage(onto_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a182d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_index.property_graph_store.save_networkx_graph(\n",
    "    name=\"OntoGraph.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4dcae",
   "metadata": {},
   "source": [
    "### Set onto graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65038e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_retriever = onto_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True,\n",
    "    top_k=20,\n",
    "    depth=3 \n",
    ")\n",
    "\n",
    "onto_engine = RetrieverQueryEngine.from_args(\n",
    "    onto_retriever,\n",
    "    include_text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ed353",
   "metadata": {},
   "source": [
    "### Test LLM based transparent retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9922945",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = onto_engine.query(\n",
    "    \"I build a ML system, what does it imply in terms of data governance ?\",\n",
    ")\n",
    "\n",
    "# Access the source nodes\n",
    "for node in response.source_nodes:\n",
    "    print(node.node.metadata)\n",
    "    # Access node content\n",
    "    print(\"**Content**\")\n",
    "    content = node.node.get_content()\n",
    "    print(content)\n",
    "    \n",
    "    # Access node score\n",
    "    print(\"**Score**\")\n",
    "    score = node.score\n",
    "    print(score)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b65be-955e-44db-942e-27e235384f19",
   "metadata": {},
   "source": [
    "# Understand knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all nodes\n",
    "all_nodes = onto_storage_context.property_graph_store.get()\n",
    "\n",
    "# Get the relation map for all nodes\n",
    "rel_map = onto_storage_context.property_graph_store.get_rel_map(graph_nodes=all_nodes, limit=400)\n",
    "\n",
    "\n",
    "# print only relations \n",
    "for rel in rel_map:\n",
    "    print(rel[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113918c-f195-4c8c-b0f3-c85d7a6c5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graph_index.get_networkx_graph()\n",
    "print(g)\n",
    "print(\"Nodes:\", g.nodes())\n",
    "print(\"Edges:\", g.edges())\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "\n",
    "with open(\"knowledge_graph.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(net.generate_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab94fc0-2080-4869-94a0-b013dc0fbd9d",
   "metadata": {},
   "source": [
    "# (Simple) Query the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93bdbb-aaa4-4658-aecf-1cd7b5038ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Quelle m√©thode utiliser pour pr√©dire si un client va faire d√©faut sur son pr√™t bancaire. Fais moi un plan.\"\n",
    "query_engine = simple_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")\n",
    "\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a871b4-70c8-45ed-a8b9-b2aaa8d15a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099031c-b137-468f-b20c-9c6fab1c4eb1",
   "metadata": {},
   "source": [
    "# (Simple) Query the knowledge graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948dd2b-acd4-4621-a2d1-b45034a21007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"Quelle m√©thode utiliser pour pr√©dire si un client va faire d√©faut sur son pr√™t bancaire. Fais moi un plan\"\n",
    "graph_query_engine = graph_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")\n",
    "\n",
    "response = graph_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5382fd-1fc8-4faf-9356-c9bb3e478db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef119d0",
   "metadata": {},
   "source": [
    "# (Simple) Query the onto graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explique moi le th√©or√®me de Bayes\"\n",
    "onto_query_engine = onto_index.as_query_engine(\n",
    " include_text=True,\n",
    " similarity_top_k=10,\n",
    ")\n",
    "\n",
    "response = onto_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582274ba",
   "metadata": {},
   "source": [
    "## (Node retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = onto_index.as_retriever(\n",
    "    include_text=False,  # include source text, default True\n",
    ")\n",
    "\n",
    "nodes = retriever.retrieve(\"Qu'est-ce que le machine learning ?\")\n",
    "\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a895520-d196-476f-b569-69a67484c120",
   "metadata": {},
   "source": [
    "# Have a real chat with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3764d-a084-4f36-9d8c-3613aa9fd835",
   "metadata": {},
   "source": [
    "## Set up the engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e55a2-a9a9-4c4e-89a4-c4dfba55ebe4",
   "metadata": {},
   "source": [
    "### Vector engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1581c90c-cf5a-40d3-81b0-086238068f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "vector_chat_engine = simple_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \" \"\n",
    "        \" \"\n",
    "        \".\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a1c22-111b-4793-8a3d-bd3eae7e3ec7",
   "metadata": {},
   "source": [
    "### Graph engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98b73cc2-0588-4b96-9aa2-1dfaf4cadb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "graph_chat_engine = graph_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897ec22",
   "metadata": {},
   "source": [
    "### Onto engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d04e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "onto_chat_engine = onto_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"\"\n",
    "        \" \"\n",
    "        \".\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e0a08-f6ff-4ee6-a165-9b9fd90621fa",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12804e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "709a37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=10000)\n",
    "chat_engine = onto_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"Tu √©cris une newsletter qui s'appelle le bateau ivre des donn√©es. Une newsletter pour apprendre √† naviguer et concevoir des dispositifs dans les mers √©tranges des donn√©es\"\n",
    "        \"Dans cette newsletter il est question d'exposer des outils, des r√©flexions ou des ouvrages mais plut√¥t de vous livrer, chaque semaine, le fruit d'une investigation au coeur d'un ph√©nom√®ne √©trange rencontr√© lors d'interactions homme-donn√©es. \"\n",
    "        \"\"\" Chaque article se construit en 5 parties : 1Ô∏è. le contexte d'√©mergence du ph√©nom√®ne, 2. l'√©v√®nement qui donne envie d'explorer le ph√©nom√®ne 3. les outils qui permettent d'explorer le ph√©nom√®ne 4.l'origine ou le d√©cryptage de la sensation d'√©tranget√© 5. les choix, solutions et outils possibles pour solutionner cette √©tranget√©\"\"\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baaede",
   "metadata": {},
   "source": [
    "## Produce content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                Tu vas m'√©crire l'introduction d'un article du bateau ivre des donn√©es qui vise √† decrypter l'√©trange fait que les mod√®les de machine learning n'ont parfois pas le comportement attendu.  \n",
    "                                                Cet article vise √† decrypter les ph√©nom√®nes de concept drift, data drift, semantic drift mais aussi les √©mergences de syst√®mes complexes\n",
    "                                                L'article vise √† r√©pondre √† cette premi√®re question : pourquoi des mod√®les de machine learning ont des comportements non-pr√©vus par leurs concepteurs ? \n",
    "                                                Puis √† cette deuxi√®me question : comment peut-on r√©interpr√©ter les r√©sultats d'un mod√®le de machine learning une fois ceux-ci produits ? comment faire une forme de r√©tro-ing√©nierie de la d√©cision du mod√®le ?\n",
    "                                                Et enfin √† cette troisi√®me question : comment peut-on se servir de ces r√©sultats pour am√©liorer le mod√®le ? \n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re la premi√®re section de l'article\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in response_stream.response_gen:\n",
    "    print(token, end=\"\")\n",
    "\n",
    "retrieved_nodes = response_stream.source_nodes\n",
    "\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.text)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6240b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re la deuxi√®me section de l'article.\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92addc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2906179",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re la troisi√®me section de l'article.\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re la quatri√®me section de l'article.\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3155041",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e52045",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re la cinqui√®me section de l'article en √©vitant les bullets points.\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                G√©n√®re une conclusion pour l'article qui r√©pond √† l'ensemble des questions pos√©es initialement.\n",
    "                                                \n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e037dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341aeefa",
   "metadata": {},
   "source": [
    "### Sum-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dcfaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = memory.get_all()\n",
    "\n",
    "# Assuming chat_history is available and contains your messages\n",
    "assistant_messages = [\n",
    "    message.content \n",
    "    for message in chat_history \n",
    "    if message.role == MessageRole.ASSISTANT  # Compare with the enum directly\n",
    "]\n",
    "\n",
    "\n",
    "output_filename = r\"/Users/arthursarazin/Documents/knowledge_glossary/output.md\"\n",
    "# Write to a Markdown file\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for msg in assistant_messages:\n",
    "        f.write(msg + \"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b195f2a",
   "metadata": {},
   "source": [
    "## Answer question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d00af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que chat_engine soit d√©j√† d√©fini avec ton LlamaIndex ou un moteur similaire\n",
    "\n",
    "# üìù Liste des prompts\n",
    "\n",
    "prompts = [\n",
    "\n",
    "    \"What are the best ways to transform data for different audiences?\",\n",
    "    \"What are your best data quality risk management strategies?\",\n",
    "    \"How do you improve data relevance for your audience?\",\n",
    "    \"How can you justify data quality investments?\",\n",
    "    \"What are the best ways to learn and share data quality practices?\",\n",
    "    \"How do you choose a data quality vendor?\",\n",
    "    \"What is your strategy for incorporating data quality dimensions and attributes?\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# üìù Fonction pour g√©n√©rer les r√©ponses\n",
    "def stream_responses(prompts, chat_engine):\n",
    "    \"\"\"It√®re sur une liste de prompts et affiche les r√©ponses g√©n√©r√©es.\"\"\"\n",
    "    for i, prompt in enumerate(prompts, start=1):\n",
    "        print(f\"\\nüîπ **Prompt {i}:** {prompt}\\n\")\n",
    "\n",
    "        # üöÄ Appelle stream_chat avec le prompt\n",
    "        response_stream = chat_engine.stream_chat(\n",
    "            f\"{prompt} Answer with one paragraph in less than 600 characters and use the first person (I...) \"\n",
    "            \"Use your knowledge base to provide a detailed response.\"\n",
    "        \n",
    "        )\n",
    "\n",
    "        # üìù Collecte la r√©ponse\n",
    "        generate = response_stream.print_response_stream()\n",
    "        \n",
    "        \n",
    "\n",
    "# üöÄ Ex√©cuter la fonction\n",
    "stream_responses(prompts, chat_engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74a302-bb28-4bf8-9103-1f62750a5fc0",
   "metadata": {},
   "source": [
    "## Sum-up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
