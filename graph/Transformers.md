Way of converting text into numerical values. It converts text to features using : 
* [[Word embeddings]]
* Positional encodings
* Attention : a measure of how strongly words in sentence are related.

This transformers can work using pre-trained models (meaning they were trained on a large corpus of texts and have a general understanding of language) or can work on a specific corpus. 

> [!image] Image of word embeddings 
![[word_embeddings.png]]



## Related concepts

- [[Attention weights]] - Core mechanism of transformers
- [[Hidden states]] - Internal representations in transformers
- [[Encoder-only]] - Architecture type (e.g., BERT)
- [[Decoder-only]] - Architecture type (e.g., GPT)
- [[BERT]] - Famous encoder-only transformer
- [[GPT2]] - Example of decoder-only transformer
- [[Large Language Model]] - Built on transformer architecture
- [[Embeddings]] - Input representation for transformers
